---
title: "Workflow for using DRIVES database tables with drivesR"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{Workflow for using DRIVES database tables with drivesR}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)

```

This vignette documents a workflow for accessing and harmonizing tables from the Diverse Rotations Improve Valuable Ecosystem Services (DRIVES) database at [drivesmachine.cals.ncsu.edu](https://drivesmachine.cals.ncsu.edu). The database contains yield and weather data from 21 long-term agricultural field experiments with a crop rotation component. You can find out more about the DRIVES project and request access at our website, [drives-network.org/](https://www.drives-network.org/). Metadata and a full data inventory can be found at the DRIVES database repository on Ag Data Commons: https://hdl.handle.net/10779/USDA.ADC.28654694.

This package has two main purposes for users interested in analyzing DRIVES data. First, it provides easy-to-use functions to download data tables from the Directus API. Second, it provides harmonization functions to combine and reshape tables for analysis. 

As of May 2025, access to DRIVES database tables requires an API key. Project members and collaborators can request an API key, which will proide full read access, from the DRIVES data manager. Other users must complete our [data use survey](https://www.drives-network.org/data-request) and agree to our data use terms to receive an API key. The latter will be assigned a role called "approved public access." This role has read access to a subset of the data excluding sites and site years embargoed from public access. 


# Installation
To install the drivesR package, use the install_github function from the devtools package. 
```{r installation, eval=FALSE}
devtools::install_github("DRIVES-project/drivesR", ref = "main")
library(drivesR)
```


# Accessing the DRIVES database
## Requesting access
 The DRIVES database is hosted at [drivesmachine.cals.ncsu.edu/](https://drivesmachine.cals.ncsu.edu/). We ask that anyone interesting in using DRIVES data complete the [data request form](https://www.drives-network.org/data-request). Direct collaborators and project members can request access directly from the DRIVES data manager. Either way, you will receive an email with a username and password.  

 You can use the username and password to log in to the [Directus Data Studio Application](https://drivesmachine.cals.ncsu.edu/admin/login), where you can view tables through the graphical interface. You can change your password through the log-in page. 

After you complete the request process, you can use your user email and password to create a personal access token (PAT) for the API. The PAT expires after 7 days and can be regenerated using a 'refresh' token. The drivesR package has an easy-to-use function for generating and refreshing tokens (see below). Approved users can request a long-term API key from the data manager. The PAT and the API key are interchangeable in the examples below. See below details on how to generate a PAT.

The PAT or API key is a string of random letters and numbers. For use with this package, the Directus API key must be formatted, "Bearer APIKEY". It is best practice never to hard-code a PAT or API key into a script. We recommend making a setup script that is sourced by other scripts (be sure to add this to your .gitignore if you're using github). Alternatively, you can use this script to read credentials from a non-indexed location.


The function generate_directus_pat() uses your user email and password to generate a PAT, then saves it in a specified location. For security, run this in the console or import your password from a non-indexed location.
```{r creds1, eval = FALSE}
generate_directus_pat(useremail = "myemail@fake.com", 
                      userpassword = "mysupersecurepassword",
                      savedir = ".", # default is the working directory.  
                      savename = "directus_PAT.txt")
```

The Directus PAT should last 7 days. You can always regenerate it.
You can use this code to read and check the validity of your PAT:

```{r creds2, eval = FALSE}
directus_token <- read_directus_pat("directus_PAT.txt"))

```
 If you have an API key, you can read it in from a non-indexed location. For example, you can source an R script with the following code, replacing MYAPI key with your string of letters and numbers:
 
```{r creds3, eval = FALSE}
directus_token <- "Bearer MYAPIKEY"

```
 


## Setup steps
If you format your setup script as described above, you should include these lines of code at the beginning of every any script that downloads DRIVES tables:

```{r silent_setup, include = FALSE, eval=TRUE}
library(drivesR)
source("directus_creds.R")
options("drivesR.default.url" = directus_url)
set_default_token(directus_token)

```

```{r setup1, eval=FALSE}
library(drivesR)
directus_token <- read_directus_pat("directus_PAT.txt")
set_default_token(directus_token)
```

The last line sets the global option for the Directus token that is used by functions in the drivesR package. You can view this option with getOption("drivesR.default.directus.token").


## Downloading DRIVES tables. 

The drivesR package has several options for accessing DRIVES database tables

1) _Download individual tables._
The *get_db_table* function uses table names as inputs. 

```{r get_db_table, eval= TRUE}
crop_info <- get_db_table("crop_info")
str(crop_info)
```

You can see the names of tables available for public access by running this code after the package is loaded (this excludes dictionary tables that are available without an API key).
```{r default.tablevec}
options("drivesR.default.tablevec")

```


2) _Download multiple tables at once._
The *import_db_tables* function accesses multiple tables at once. This can be used in several ways depending on your workflow and system limitations. 

The argument "tablevec" determines which database tables are downloaded. By default, the function downloads all tables in *getOption("drivesR.default.tablevec")*. If set to NULL, the function will download all tables available to your user role. You can also provide a vector of table names. 
```{r import_db_tables1, eval=FALSE}
db <- import_db_tables(tablevec =  c("crop_yields","site_info","experimental_unit_treatments"))
```
For the remaining examples, we'll assume the default "tablevec" option. 

The argument "fetch_option" controls whether tables are imported as a list to the global R environment ("download.only"), saved to a specified directory ("save.only"), or both ("download.save"). It also has the option to read tables from a local directory after they have been downloaded ("upload"). The latter option allows you to rerun the same code without having to repeat the time-consuming step of downloading the tables. For example, you could run the following code to download tables into a list in your global R environment and saved as an RData file on your local directory:  

```{r import_db_tables2, eval=FALSE}
db <- import_db_tables(fetch_option = "download.save",
                       #fetch_option = "upload",
                       savedir = "data",
                       savename = "sampledblist")
```

Then, you can move the comment and rerun the same code to load the tables from the local directory:

```{r import_db_tables3, eval=TRUE}
db <- import_db_tables(#fetch_option = "download.save",
                       fetch_option = "upload",
                       savedir = "data",
                       savename = "sampledblist")
```

Importing tables into a list in the global R environment is convenient for downstream analysis, but can take up a lot of working memory. If working memory is limiting, you can bypass this step by setting the fetch_option as "save.only". 
  The save_option argument allows you to specify how you want the tables saved. The default is an R data object with a list of tables ("list"). You can also save tables as separate rds or csv files. For systems with limited working memory, it may work better to save tables as separate files and load individual tables as needed.    

Users who plan to work with the data tables outside of R may want to save the tables into separate CSV files locally:
```{r import_db_tables4, eval=FALSE}
db <- import_db_tables(fetch_option = "save.only",
                       save_option = "csv",
                       savedir = "myChosenFolder",
                       savename = "drives" 
                       )
```

## Database metadata
The database contains three data dictionary tables with detailed descriptions about tables, columns within tables, and categorical variables. The column dictionary includes the details used to generate the database schema, including primary keys and foreign key relationships. 

Dictionary tables can be downloaded individually, using the *get_db_table* function, or all at once as a list with *import_dictionary_tables*.
```{r import_dictionary_tables, eval=FALSE}
## separate:
table_dictionary <- get_db_table("table_dictionary")
column_dictionary <- get_db_table("column_dictionary")

## together:
dict <- import_dictionary_tables()
```

If you're interested in seeing how the database is structured in Directus, you can use the *get_db_info* function to view metadata for any endpoint in the API. The example below shows metadata for the table dictionary. The output can be formatted as a json or a dataframe. The json option is a clearer representation of how information is structured in Directus. See [https://directus.io/docs/api](https://directus.io/docs/api). For more information on the Directus API. 

```{r get_db_info, eval = TRUE}
table_dict_info <- get_db_info(mytarget = "collections/table_dictionary",
                             output_format = "json")
print(table_dict_info)
```
One detail to note is that non-dictionary tables with approved public viewer access have a prefix "public_". This only comes up when viewing tables in the [Directus Studio App](https://drivesmachine.cals.ncsu.edu/admin/login) or specifying API endpoints directly, as shown below. 

# Harmonizing DRIVES tables
The DRIVES database was designed to be flexible for various end-uses and preserve as much information as possible within each experiment. The relational database design is useful for these objectives, but adds a layer of difficulty for analyzing the data. Therefore, the drivesR package provides data harmonization functions that reshape and combine data tables to ease downstream analysis. Each function has arguments to facilitate different end uses. We start with lower-level harmonization functions that preserve information in individual tables, then higher level functions that combine tables with some loss of information.


### 1) Crop yields:
The *harmonize_yields* function performs useful operations on the crop_yields table. If the table is already downloaded, it can be used as an argument. Otherwise, the function downloads the table using the Directus API. 

```{r harmonize_yields, eval=FALSE}
## without pre-downloaded tables:
yields <- harmonize_yields()
## with pre-downloaded tables:
crop_yields <- get_db_table("crop_yields")
yields <- harmonize_yields(crop_yields)
## or, if in a list:
yields <- harmonize_yields(db$crop_yields)
```

The function performs some light processing steps, such as calculating dry yields and adding a TRUE or FALSE column for cover crops. It also provides the option to keep crop fractions from the same crop/unit/year (e.g. grain and straw) in separate rows, to rearrange them into columns. 
```{r harmonize2, eval = TRUE} 
## crop fractions in rows
#crop_yields <- get_db_table("crop_yields")
yields <- harmonize_yields(db$crop_yields) # could be a separate dataframe or part of a list
longyields <- harmonize_yields(db$crop_yields,
                               crop_fractions_as_columns = FALSE)
dim(longyields)
names(longyields)
## crop fractions in columns
wideyields <- harmonize_yields(db$crop_yields,
                               crop_fractions_as_columns = TRUE)
dim(wideyields)
names(wideyields) # columns include suffix _1 for the primary fraction and _2, _3, etc. for other fractions.
```


## 2) Experimental treatments and design:
Information about experimental treatments and design is organized across multiple tables in the DRIVES database. Some of these tables are useful to interact with directly. Others require harmonization to be useful for analysis.  

- **site_treatment_type_info**: Provides information about types of experimental treatments at a site level. Treatments correspond to a core set of management practices described for all sites. The table includes textual descriptions and information about how the treatments factor into the experimental design. This is a useful starting point for evaluating management practices represented in the DRIVES database and possibilities for analysis. 

- **site_treatment_level_info**: Provides information about the levels within each treatment within *site_treatment_type_info*).  

- **rotation_id_info**: Since the DRIVES project is centered around crop rotations, rotation information has its own set of tables separate from other treatments. Each site-specific rotation is assigned a unique rotation_id, described in this table, along with useful overview information such as the length, species richness, and inclusion of cover crops or perennials. This rotation_id is included in *treatment_id_components*.  

- **rotation_phases**: Provides detailed information about the cropping sequence for each rotation_id. This includes crop identifiers, timing of planting and harvest, crop functions (e.g., grain, cover), and what is done with different crop fractions (whether they're measured, removed, retained). 

- **treatment_id_info**: Combinations of practices applied to the same experimental unit are assigned a unique treatment_id. There are two types of treatment ids: those assigned based on management practices alone (treatmentID1), and those assigned based on management practices plus staggered rotation phases (treatmentID2). This table lists each treatment identifier, its type, and its corresponding site_id. It mainly exists to be merged into the next table. 

- **treatment_id_components**: Specifies the component treatment levels for each treatment_id. The treatment levels relate back to site_treatment_level_info and rotation_id_info. The table is structured with a start and end year to each treatment components to accommodate changes over time.  

- **experimental_unit_info**: Provides spatial and design information about experimental units. Information includes the type of unit (split-plot, plot, block, etc.), relative position within the field, directional orientation, and any parent units (for split-plots within plots, plots within blocks. etc.). 

- **experimental_unit_treatments**: Indicates which treatment_ids were applied to which experimental units in which years, organized as start year and end year.  

The *harmonize_treatments* function takes information from *treatment_id_info* and *treatment_id_components* and expands them into a time-series data set with one row per treatment_id/year and separate columns for each treatment component. This can be useful for exploratory analysis of experimental designs. The function accepts a list of database tables named *treatment_id_info* and *treatment_id_components*. If those are not provided, it fetches those tables from the Directus database.
```{r harmonize_treatments, eval=FALSE}
## tables provided
trt <- harmonize_treatments(db) # db is a named list with 'treatment_id_info' and 'treatment_id_components'

# tables not provided
trt <- harmonize_treatments()

```


The harmonize_treatments_units function produces a dataframe with one row per unit_id and year, and separate columns for each treatment id and component. This structure allows it to be merged with yield data. Similar to other harmonization functions, it accepts a named list or fetches tables from the database.
```{r harmonize_treatments_units, eval=FALSE}
## tables provided
trtunit <- harmonize_treatments_units(db) # db is a named list with 'treatment_id_info', 'treatment_id_components', and 'experimental_unit_treatments'

# tables not provided
trtunit <- harmonize_treatments_units()


```

The full dataframe with all treatment components may be a little daunting. We made a function list_treatment_types_by_management_practice to help subset column names corresponding to particular management practices. This uses information in the site_treatment_type_info table. The function accepts this table as an argument, or fetches it from the database. 

```{r list_treatments_by_management_practice, eval = TRUE}
trt <- harmonize_treatments(db)
trtlist <- list_treatments_by_management_practice(db)
# lets say I wanted to extract information on N fertility treatments
trtcolz <- trtlist$`N fertility`
trtsubset <- trt[c("site_id","year", trtcolz)]
head(trtsubset)

```

## 3) Management data
Currently, the DRIVES database has two tables with time-series management details. 

- *planting_info* provides planting dates, variety identifiers, and seeding rates for every unit/year/crop where available. It includes separate rows for components of crop mixtures and for replanting, if earlier plantings failed. This level of detail makes the data a little difficult to work with.

The harmonize_planting_info function offers several options to simplify the planting_info table into one row per unit/year/crop/date. 
Multiple planting dates can be retained in separate rows or columns, or trimmed to include only the latest planting date. Component crops within mixtures can be retained as separate columns or excluded. See ?harmonize_planting_info for details on options and output. 

Like other harmonization functions, it can accept the planting_info table as an input or fetch it from the database.
```{r harmonize_planting_info, eval=TRUE}
#planting_info <- get_db_table("planting_info") # can be a separate table or part of a list.
p1 <- harmonize_planting_info(db$planting_info, replant_dates = "latest",include_component_crops = TRUE)
dim(p1)
names(p1)# names have a numbered suffix for component crops in mixtures.
# And since replant_dates = 'latest', there is an added column replantedTF indicating whether a unit was replanted.

p2 <- harmonize_planting_info(db$planting_info, replant_dates = "rows",include_component_crops = FALSE)
dim(p2)
names(p2) # All planting dates are included and organized under a date_index column. Since include_component_crops is FALSE, there is an added column num_components providing the number of component crops within each actual_crop_id.

p3 <- harmonize_planting_info(db$planting_info, replant_dates = "columns",include_component_crops = TRUE)
dim(p3)
names(p3) # names include suffixes d and c for planting dates and component crops, respectively.

```

*harvest_dates* provides harvest dates for every unit/year/crop where available. It includes separate rows for multiple harvest dates (typical for forages) and for crop fractions harvested on different dates (rare). 

The harmonize_harvest_dates function reshapes the harvest_dates table so multiple harvests are in separate columns (one row per unit/year/crop). It has an option to put crop fractions with different harvest dates in separate columns (rarely an issue).

Like other harmonization functions, it can accept the harvest_dates table as an input or fetch it from the database
```{r harmonize_harvest_dates, eval=TRUE}
#harvest_dates <- get_db_table("harvest_dates")# can be a separate table or part of a list.
h1 <- harmonize_harvest_dates(db$harvest_dates, crop_fractions_as_columns = FALSE)
dim(h1)
names(h1)

h2 <- harmonize_harvest_dates(db$harvest_dates, crop_fractions_as_columns = TRUE)
dim(h2) # only slightly different.
names(h2)
```

## 6) Weather
The DRIVES database contains two tables pertaining to weather:
 **weather_daily** contains daily weather station data for min and max precipitation and cumulative precipitation. Missing observations are supplied from gridded data from [Daymet](https://daymet.ornl.gov/). The table is organized in a long format, with weather variables in separate rows.
 **weather_station_info** provides information on weather station identifiers used in the weather_daily data. This includes their lat-lon coordinates and urls for data access (if applicable). 

The weather table is in wide format and ready for analysis. Weather variables can be merged with other datasets based on site_id and year. It is up to the user to coalesce between daily and yearly time scales. 

## 5) Putting it all together
The *harmonize_yields_treatments* function combines the harmonization steps for treatments, units, and yields and merges them together. It includes the option to keep crop fractions in separate rows or reorganize them into columns. It can operate on a list of pre-downloaded tables, or fetch the tables from Directus. 

```{r harmonize_yields_treatments, eval=TRUE}
# db <- import_db_tables(c("treatment_id_info","treatment_id_components","experimental_unit_treatments","crop_yields"),fetch_option = "download.only")
hyt <- harmonize_yields_treatments(db, crop_fractions_as_columns = TRUE) 
names(hyt) #names will have numbered suffixes for crop fractions.
dim(hyt)

hyt <- harmonize_yields_treatments(db, crop_fractions_as_columns = FALSE)  
names(hyt) 
dim(hyt)

```

The *harmonize_yields_planting_harvest* function combines the harmonization steps for yields, planting info, and harvest dates. The only argument is the input list of data tables, which are fetched from Directus if NULL. Details on component crops and multiple planting dates from planting_info are excluded. Multiple harvest dates are included as separate columns, as in *harmonize_harvest_dates*. Multiple fractions per crop are included as separate columns with suffixes _f1, _f2, etc. 

```{r harmonize_yields_planting_harvest, eval=TRUE}
names(db) # full list including yield, planting, and harvest data. 
yph <- harmonize_yields_planting_harvest(db)
dim(yph)
names(yph)
```


The *harmonize_yields_planting_harvest_treatments* function combines the harmonization steps for yields, planting info, harvest dates, and experimental treatments to create one giant data table. The only argument is the input list of data tables, which are fetched from Directus if NULL. If don't mind losing some details, you can skip all the previous data download and harmonization steps (excluding weather data) and just use this function. The code below fetches the tables from the database and puts them all together. You can provide a list of previously downloaded database tables as an input.

```{r harmonize_yields_planting_harvest_treatments, eval=TRUE}
monsterdf <- harmonize_yields_planting_harvest_treatments(db)
dim(monsterdf)
names(monsterdf)
```


